Skils:
SQL, Pyspark, Databricks, Spark, ADF, ADB, Synapse, ADLS, Blob, Python(basic coding), ETL


To address your scenario where resources come in sequentially, and each time a new resource is processed, it should receive a new unique `resource_id` without reusing the previous resource's ID, you can utilize a sequence of operations in PySpark to assign unique IDs to resources based on their arrival order.

Here's an example of how you might achieve this in PySpark:

### Problem Breakdown:
1. **Resource Arrivals**: Resources come in sequentially, and each resource should be assigned a unique `resource_id`.
2. **Avoid Reuse of Previous `resource_id`**: When a new resource comes in, it should not take the same `resource_id` as the previous one.

### PySpark Code:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import monotonically_increasing_id

# Initialize Spark session
spark = SparkSession.builder.appName("ResourceIDAssignment").getOrCreate()

# Sample input data: resource arrival in order
data = [
    ("resource1", "2024-11-18 10:00:00"),
    ("resource2", "2024-11-18 10:05:00"),
    ("resource3", "2024-11-18 10:10:00"),
    ("resource4", "2024-11-18 10:15:00")
]

# Create a DataFrame from the input data
columns = ["resource_name", "arrival_time"]
df = spark.createDataFrame(data, columns)

# Add a unique id to each row based on the arrival order (monotonically_increasing_id will ensure a unique number)
df_with_ids = df.withColumn("resource_id", monotonically_increasing_id())

# Show the result
df_with_ids.show(truncate=False)

# Output:
# +------------+-------------------+----------+
# |resource_name|arrival_time        |resource_id|
# +------------+-------------------+----------+
# |resource1   |2024-11-18 10:00:00 |0         |
# |resource2   |2024-11-18 10:05:00 |1         |
# |resource3   |2024-11-18 10:10:00 |2         |
# |resource4   |2024-11-18 10:15:00 |3         |
# +------------+-------------------+----------+
```

### Explanation:
1. **Input Data**: The data consists of a list of resources, each associated with an arrival time. This order represents when the resource arrives in the system.
2. **`monotonically_increasing_id()`**: This function generates a unique, sequential identifier for each row. It is not guaranteed to be strictly sequential, but it ensures each row gets a distinct identifier that is increasing.
   
   - Note that `monotonically_increasing_id()` produces a globally unique ID that may have gaps between consecutive values (since the ID is distributed across partitions in Spark). However, it guarantees uniqueness within the context of a DataFrame.
3. **`withColumn`**: We use `withColumn` to create a new column `resource_id` in the DataFrame that is generated by `monotonically_increasing_id()`.

### Output:
Each resource will receive a unique `resource_id` in the order they arrived. This ensures that new resources will not reuse the IDs of previous resources.

### Further Improvements:
- If you need sequential, gap-free `resource_id`s (e.g., starting from 1), you could use `row_number()` or `monotonically_increasing_id()` with `Window` functions to ensure a more sequential order.

Here is an example using `row_number()`:

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

# Define a window specification to order by arrival time
window_spec = Window.orderBy("arrival_time")

# Generate sequential IDs starting from 1
df_with_sequential_ids = df.withColumn("resource_id", row_number().over(window_spec))

# Show the result
df_with_sequential_ids.show(truncate=False)

# Output:
# +------------+-------------------+----------+
# |resource_name|arrival_time        |resource_id|
# +------------+-------------------+----------+
# |resource1   |2024-11-18 10:00:00 |1         |
# |resource2   |2024-11-18 10:05:00 |2         |
# |resource3   |2024-11-18 10:10:00 |3         |
# |resource4   |2024-11-18 10:15:00 |4         |
# +------------+-------------------+----------+
```

In this case, `row_number()` assigns a strictly increasing, gap-free ID starting from 1, based on the `arrival_time` column.

This approach ensures that each resource gets a unique `resource_id` that isn't reused and follows the arrival order.

---incendo
emp, salary, dept,year

1,1000,hr,2020
2,1000,hr,2020
3,1200,hr,2020

1,1500,hr,2021
2,1000,hr,2021


1,2000,hr,2022

2,1000,hr,2021
2,1500,hr,2022

find empid of such employees who received hikes in 2 consecutive years

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark session
spark = SparkSession.builder.appName("SalaryHike").getOrCreate()

# Sample data
data = [
    (1, 1000, 'hr', 2020),
    (2, 1000, 'hr', 2020),
    (3, 1200, 'hr', 2020),
    (1, 1500, 'hr', 2021),
    (2, 1000, 'hr', 2021),
    (1, 2000, 'hr', 2022),
    (2, 1000, 'hr', 2021),
    (2, 1500, 'hr', 2022)
]

# Create DataFrame
columns = ['empid', 'salary', 'dept', 'year']
df = spark.createDataFrame(data, columns)

# Define window spec to sort by empid and year
window_spec = Window.partitionBy('empid').orderBy('year')

# Calculate salary difference for each employee between consecutive years
df_with_diff = df.withColumn('salary_diff', F.col('salary') - F.lag('salary', 1).over(window_spec))

# Flag whether there was a salary hike
df_with_hike = df_with_diff.withColumn('salary_hike', F.when(F.col('salary_diff') > 0, 1).otherwise(0))

# Check if the employee received hikes in two consecutive years
df_with_consecutive_hike = df_with_hike.withColumn('consecutive_hike',
    F.col('salary_hike') & F.lag('salary_hike', -1).over(window_spec)
)

# Filter employees who received a hike in two consecutive years
result = df_with_consecutive_hike.filter(F.col('consecutive_hike') == 1).select('empid').distinct()

# Show the result
result.show()


write a python program to find the count of duplicate elements in a list 
L1 = [ 2,4,8,9,2,7,6,7,2] 
o/p : 2- 3 7-2

Live Coding Board
Saved

CANGRA Talents


--gavs:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder.appName("IncrementalLoadExample").getOrCreate()

# Sample existing data (destination data, already loaded previously)
existing_data = spark.createDataFrame([(1, "Aarthi")], ["id", "name"])

# New incoming data (updated records)
new_data = spark.createDataFrame([(1, "Aarthik")], ["id", "name"])

# 1. Identify records to be updated (existing id in the destination)
updated_data = existing_data.alias('existing').join(
    new_data.alias('new'), 
    on=['existing.id'], 
    how='inner'
).filter(col('existing.name') != col('new.name')).select('new.id', 'new.name')

# 2. Identify records to be inserted (id in new data but not in existing data)
new_inserts = new_data.join(existing_data, on=["id"], how="left_anti")

# 3. Combine the records that need to be inserted and updated
combined_data = updated_data.union(new_inserts)

# 4. If you're using a file-based approach, overwrite the data
# Write the updated data back to the destination (for simplicity, writing as a new DataFrame here)
final_data = existing_data.join(updated_data, on="id", how="left_anti").union(updated_data).union(new_inserts)

final_data.show()


----how to join 3 list in single dataframe
df_merged = df1.join(df2, on='id', how='inner').join(df3, on='id', how='inner')


----teams 
what is unity catalog in adb
how to configure email if pipeline is failed
write pyspark function
how to create parameter

what is workflow
 
how to run new notebook from another notebook
 
CI/CD
can we create 2 primary key _ yes that is composite key,magic table/query in sql,how to improve performance,how to global parameter in deployment,
magic command for pyspark,acid,how to avoid sql injection,how to find which query run more time any tool,transactions, 
can we use dml/ddl statement inside transactions,lead,lag,difference between delete,truncate,cte,type of containers,type of error u faced in ssis,

API

why delete is slower than truncate,lookup,how to do incremental load for insert,update, h
ow to find duplicate using autoid, what is impact of delete,truncate in autoid column,logic app,indexes,dynamic query,difference between variable,parameter,
 
what will be u use to improve performance in ur query
 

why delete is slower than trucate
 
index drawback-- disc space more,insert,update,delete slower
 
do u use dynamic query in ur project, cte vw subquery,why subquery is faster that cte
 
list function in sql, materialized view,Schema binding view

volume in dbfs,

write sql/pyspark query for below table
Aarthi      1
Aarthi      2
Karthick    3

o/p like below:
Aarthi 1,2
karthic 3

agg--concat() groupby
 
join 3 tables based on id in single dataframe in pyspark
 
what are parameter are in cluster creation
 
final write statement in pyspark,spark sql
 

how to archive files in adb 
sql stored procedure --- u will convert into pyspark or spark sql,which is better
 
what is use of unity catalog
 
characterisitcs of pyspark
 
parquet,avro,json which is better, are u using it
 
abfs ?
 
dbfs?
 
bronze,silver,medallion layer

what is storage key in adf
 
storgage key type
 
serialization in rdd,pyspark
 
how to improve performance in adb
 
difference between rdd,dataframe,datasets

pip
 
how to handle missingvalue in python,how to handle large datasets,how to create job clusters, workflows, how to hid some specific column in table to downstream
 
repartition
 
how to improve pyspark performance
 
how to clean the data in sql,pyspark
 
how to handle missing values in python
 
union,joins in python,sql,pyspark
 
what is spark session
 
findstring,

instring,substring --sql
 
how to fetch even numbers in sql,pyspark
 
how to fetch column which are null in pyspark
 
fetch only first 5 rows in column
 
how to find schema of dataframe in pyspark,printschema
 
Mounting integrates external storage into the Databricks workspace by creating a mount point, which is a path under DBFS. This allows you to interact with external storage in the same way you interact with local filesystem paths in Databricks.\

difference between IR,linked service,Dataset

there is 10 file, u have to delete file which size more than 10 mb ---get metadata then use if condition activity in adf

how to do incremental load in ADF and in ADB
 
cluster manager, spark architechture, job orchestration
 
spark context,spark session,
 
spark internal working
 
print duplicates in python(for, empytlist), 3 highest salary in sql,pyspark--use dense rank
 
are u using batch or streaming data, how to do incremental load (based on one column) in adf
 
difference between jobs,tasks,stages in spark

Spark Architecture: driver,cluster manager,worker,executor,task
Pyspark architecture:control and dataplane
 
Key Concepts
Resilient Distributed Dataset (RDD)
RDD is the foundational abstraction in Spark. It represents an immutable distributed collection of objects that can be processed in parallel. RDDs are lazily evaluated, meaning transformations are defined but computations are only executed when actions are invoked.
DataFrames and Datasets
DataFrames: High-level abstraction that provides schema-based operations and SQL-like querying capabilities.
Datasets: A type-safe, object-oriented abstraction over RDDs, with additional optimizations.
Transformations and Actions
Transformations: Define a computation but are lazily evaluated.
Examples: map, filter, reduceByKey
Actions: Trigger computations and return results.
Examples: count, collect, saveAsTextFile

Fault Tolerance
Spark provides fault tolerance by maintaining data lineage through RDDs. RDDs track the sequence of operations applied to data. If a partition of data fails, Spark can recompute only the failed partition from its lineage (original data and transformations applied).

difference between rdd,dataframe,dataset
 
difference between web and webhook actitivty in adf
 
vacuum in adb
 
difference between azure,self hosted,azure ssis IntegrationRuntime in adf
 
types of cluster,unity catalog
 
read mutilple json file and do incremental load
 
how to define schema for  [{"one":1,"two":[1,2,3]}] use maptype
 
cache vs persist
 
metadata pipleine in cicd or meta data pipeline
 
why webactivity is used
 
how jobs works in spark
 
incremental load
 
parquet file
 
delta table
 
datawarehouse olap,oltp

define schema for below code snippet:
schema=???
spark.createDataFrame([{"one":1,"two":[1,2,3]}],schema=schema)

schema=["key","value"]
schema=structtype [
structfield(key,stringtype()),
structfield(value,Maptype(stringtype(),Arraytype()))]

map(),flatmap() in python,pyspark

struct type vs maptype

cache and persist in pyspark

magic commands in adb

lead,lag,list function in sql

read csv file in pyspark, remove duplicate in that,create new column bonus with 1000 (withcreate),filter null values

df=spark.read.csv("path").option(header,true)
df.distinct() or df.dropduplicates
df1=df.withcreatecolumn("bonus",lit("1000"))
df1.filter(col(name).isnull)--refer original syntax once

how to do optimization in pyspark(broadcast join, drop duplicate,handle null value,missing value,use colaesence instead of reparition,ext)

how to do query optimization in sql

how to avoid sql injection

acid 

coalescence vs repartition

type of partitioning, memory and disk partitioning(memory--coalescence(),repartition())(disk--partitionby())

while reading csv file which portioning will come to ur mind?

webactivity vs webhook activity

delta table in azure data bricks---very important

To calculate the running total of sales for each product in a given month, along with the rank of each product based on total sales.
salesid, prodid, salesdate, prod name


SQL Query to do sum on quarter wise
purch_amt   ord_date    
----------  ----------  
150.5       2013-01-05  
270.65      2012-03-05  
65.26       2012-12-05  
110.5       2015-10-06  
948.5       2012-05-06  
2400.6      2012-10-07  
5760        2017-08-07  
1983.43     2012-07-07  
2480.4      2012-10-07


data = [('2013-01-05', 150.5), ('2012-03-05', 270.65), ('2012-12-05', 65.26),
        ('2015-10-06', 110.5), ('2012-05-06', 948.5), ('2012-10-07', 2400.6), ('2017-08-07', 5760.00), ('2012-07-07', 1983.43), ('2012-10-07', 2480.4)]
schema = StructType([StructField('orderdate', StringType(), True),\
                     StructField('amount', FloatType(), True)])
#schema = ["ord_date"['date'], "purch_amt"['float']]
df = spark.createDataFrame(data, schema)
df.display()

query:
with cte1 as (
select amount, orderdate,
month(orderdate) as month
from data
),
cte2 as (
select amount, orderdate,
case when month in (1,2,3) then 'Q1'
    when month in (4,5,6) then 'Q2'
    when month in (7,8,9) then 'Q3'
    when month in (10,11,12) then 'Q4'
end as quarter
from cte1
)
select quarter, sum(amount)
from cte2
group by quarter

Pyspark code:
df = df.withColumn('quarter', date_format('orderdate','MM'))
df2= df.withColumn('Quat', when(col('quarter').isin('01','02','03'), 'Q1').when(col('quarter').isin('04','05','06'), 'Q2').when(col('quarter').isin('07','08','09'), 'Q3').otherwise('Q4'))
df2.display()
df2.groupBy('Quat').agg(sum('amount').alias('TotalSum')).display()


Q1:
select * 
from t1
join t2 on t1.id = t2.id and t1.name = t2.name
join t3 on t2.id = t3.id and t2.name = t3.name

t1 ID - 10 name, - naresh
t2 ID - 10 name, - naresh
t1 ID - 20 name, - naresh

What will be the resutl for the above select query.
Ans:
Untill the first join it will give
id name   id name
10 naresh 10 naresh
But from second join it wont give any results, because innerjoin won't show nulls, only prints if all conditions equal


PWC interview questions:
emp - id name manid
      1  arun  null  
      2 paul  	1
      3 deepika 2
      4 ankita 	3
Q1. find heirachy of the manager for the employee
Ans:
select
from emp e1.name,e2.name,e3.name and so.. on..
left join emp e2 on e1.id = e2.manid
left join emp e3 on e2.id = e3.manid


Q2. find the 5th highest salary in between age of 45 and 55
Ans:
select empid
from emp
where age between 45 and 55
order by salary desc
Offset 4 row
Fetch next 1 row only

Q3. explain rank() and dense_rank()
Ans:
   rank denserank
10 1  	1
20 2	2
20 2	2
30 4	3

Q4. Write sql query to find the 5th highest salary in between age of 45 and 55
Ans:
with cte as (
select
id, age
dense_rank() over (order by salary) as rank
from emp
where age between 45 and 55
)
select id from cte where rank = 5

Q5. Delete duplicates from the below
city1 city2 disatnce
ban   che   300
che   ban   300
kol    mum  2000
mum   kol    2000


Q6.
Explain me how you design a pipeline by considering the below activities
1. when ever the data comes the job should trigger and load only incremental data
2. On success full of copy activity - the data should load to folder1 in destination
3. On failure of activity - the data should load to folder2 and send notification as failure with failure details
4. The data conatains PII data - this needs to be masked while the data load process.

Q7 Differences between coalesce() and repartition()
Q8 Differences betweeb RDD and DF


Quadrant interview questions:

Q1.
Orders with columns OrderID, CustomerID, OrderDate, and TotalAmount, 
Customers with columns CustomerID and CustomerName. 

Write a query to find the total revenue generated by each customer for the current year,
but only include customers who have placed more than 3 orders.


ADF interview Questions:

1. What are the main components of Azure Data Factory (ADF)?
2. How does Azure Data Factory differ from SQL Server Integration Services (SSIS)?
3. What are Integration Runtimes in ADF, and what are the different types?
4. What is the purpose of activities in ADF, and what are some key activities you have used?
5. How does ADF handle parallelism and concurrency in pipelines?
6. What are Linked Services and Datasets in Azure Data Factory?
7. What are the differences between Mapping Data Flow and Wrangling Data Flow in ADF?
8. How do you implement incremental data loading in ADF?
9. What are triggers in ADF, and how do they work?
10. How do you handle failure and retries in ADF pipelines?
11. What is a Self-hosted Integration Runtime, and when would you use it?
12. How can you monitor and debug pipelines in Azure Data Factory?
13. What is the use of the Copy Activity in ADF, and how can you optimize it?
14. How do you integrate Azure Data Factory with Databricks or Synapse Analytics?
15. What are tumbling window triggers, and how are they different from schedule triggers?
16. How can you handle schema changes in Azure Data Factory?
17. What are the key features of ADF v2 compared to ADF v1?
18. How can you secure data during transfer in Azure Data Factory?
19. How can you optimize the performance of data movement in ADF?
20. How do you handle dynamic content and parameters in Azure Data Factory pipelines?

1.     What are Integration Runtimes in Azure Data Factory? How do you decide between Azure, Self-Hosted, and SSIS runtimes?
2.     What is the process to create and execute a mapping data flow in Azure Data Factory? How does it differ from copy activities?
3.     Explain the process of parameterizing pipelines in Azure Data Factory. What are the benefits of using parameters?
4.     How can Azure Data Factory integrate with Synapse Analytics to execute SQL scripts or stored procedures?

5.     When should you choose Azure Functions over Azure Data Factory for data transformations and pipeline orchestration?
6.     How does Azure Data Factory handle real-time data streaming, and what Azure services can it integrate with to achieve this?

7.     What are the differences between Azure Blob Storage and Azure Data Lake Storage? When would you use each?
8.     How does hierarchical namespace impact performance and management in Azure Data Lake Storage Gen2?
9.     How does Azure Monitor integrate with Azure Data Engineering services to provide observability and monitoring for data pipelines?
10.  What is the role of Azure Purview in managing data governance and lineage for Azure data pipelines?
11.  What are some key differences between Azure Databricks and Azure Synapse Analytics when building big data solutions?

https://www.linkedin.com/pulse/10-scenario-based-azure-data-factory-interview-questions-sarkar-bvlkc/?trackingId=PdCMuNM5RmSgxYvQlECuwQ%3D%3D
https://www.projectpro.io/article/azure-data-factory-interview-questions-and-answers/601

Accenture interview questions

Introduce yourself.
What is your project architecture?
What all activities you do in adf?
What is linked service?
What all linked services you have created?
What is integration run time in adf and types of it ? 
What all tables you have created in adb?
Diff b/w dimensions and facts?
Explain types of dimensions you have worked? 
Explain types of facts you have worked?
Distinguish type0,1,2 dimensions 
What is fact less facts?
What all transformtions you have used in databricks?
What transformation is needed to remove duplicates from a dataframe?
How to get distinct records from a dataframe in pyspark?
Explain how you handle type2 dimensions. What all conditions you use in merging the type2 dimensions?
What all window functions you have worked.
And  two live coding questions asked to solve using SQL/pyspark

*
*
*
*
*
*
*
*
*

Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and retain only the latest entry based on a timestamp column?

data = [("101", "2023-12-01", 100), ("101", "2023-12-02", 150), 
        ("102", "2023-12-01", 200), ("102", "2023-12-02", 250)]
columns = ["product_id", "date", "sales"]

df = spark.createDataFrame(data, columns)
df.display()

code: This is somewhat correct
df.orderBy('date',ascending=False).dropDuplicates(subset=['product_id']).display()


Q2 You need to calculate the total number of actions performed by users in a system. How would you calculate the top 5 most active users based on this information?
data = [("user1", 5), ("user2", 8), ("user3", 2), ("user4", 10), ("user2", 3)]
columns = ["user_id", "actions"]

Code:
df = groupBy('user_id').agg(sum('actions').alias('TotalActions')).orderBy('TotalActions',ascending = False).limit(5)


6. While processing sales transaction data, you need to identify the most recent transaction for each customer. How would you approach this task?
data = [("cust1", "2023-12-01", 100), ("cust2", "2023-12-02", 150),
        ("cust1", "2023-12-03", 200), ("cust2", "2023-12-04", 250)]
columns = ["customer_id", "transaction_date", "sales"]


code: we can use window function here by importing winow library (dense_rank())



7. You need to identify customers who havenâ€™t made any purchases in the last 30 days. How would you filter such customers?
data = [("cust1", "2025-12-01"), ("cust2", "2024-11-20"), ("cust3", "2024-11-25")]
columns = ["customer_id", "last_purchase_date"]

code: we can use datediff here with current data and then we can apply filter as grater than 30 days


8. While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this?

data = [("customer1", "The product is great"), ("customer2", "Great product, fast delivery"), ("customer3", "Not bad, could be better")]
columns = ["customer_id", "feedback"]

code:
df=df.withColumn('feedback',explode(split('feedback', ' ')))
df.groupBy('feedback').agg(count('feedback').alias('feedback_count')).orderBy(desc('feedback_count')).display()


Important:
9. You need to calculate the cumulative sum of sales over time for each product. How would you approach this?
data = [("product1", "2023-12-01", 100), ("product2", "2023-12-02", 200),
        ("product1", "2023-12-03", 150), ("product2", "2023-12-04", 250)]
columns = ["product_id", "date", "sales"]

code:
from pyspark.sql.window import *
df=df.withColumn('date', col('date').cast(DateType())).orderBy(asc('product_id'),asc('date'))
df.withColumn('cum_sum', avg('sales').over(Window.partitionBy('product_id').orderBy('date'))).display()








